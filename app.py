# -*- coding: utf-8 -*-
"""Untitled30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FUFubhsxeiQL5bR4cCuIQDehp8LOxEOc
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# Replace 'Fraud.csv' with the actual filename if it's different
df = pd.read_csv('Fraud.csv')


# Display the first few rows of the dataframe
print(df.head())

# Get a summary of the dataframe's columns and data types
print(df.info())

# Get descriptive statistics for numerical columns
print(df.describe())

# Check for missing values
print("Missing values before numeric conversion:\n", df.isnull().sum())

# Convert 'amount' and 'newbalanceOrig' columns to numeric, coercing errors
df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
df['newbalanceOrig'] = pd.to_numeric(df['newbalanceOrig'], errors='coerce')

# Check for missing values after numeric conversion
print("\nMissing values after numeric conversion:\n", df.isnull().sum())

# Handle missing values (if any) - You might need to decide on a strategy (e.g., median imputation, dropping rows)
# For demonstration, we'll just check for NaNs introduced by coercion.
# If there are many NaNs introduced, you might need a more sophisticated approach.

# Identify and handle outliers (e.g., using box plots)
plt.figure(figsize=(10, 6))
sns.boxplot(x=df['amount'])
plt.title('Box Plot of Transaction Amounts')
plt.show()

# Detect and handle multi-collinearity
# Create a correlation matrix to check for highly correlated features
correlation_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Check the distribution of transaction types
plt.figure(figsize=(8, 6))
sns.countplot(x='type', data=df)
plt.title('Distribution of Transaction Types')
plt.show()

# Analyze the number of fraudulent vs. non-fraudulent transactions
print(df['isFraud'].value_counts())

# Visualize the fraudulent transactions by type
plt.figure(figsize=(8, 6))
sns.countplot(x='type', data=df[df['isFraud'] == 1])
plt.title('Fraudulent Transactions by Type')
plt.show()

# 1. Import required libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# 2. Load the dataset
# Make sure to use the correct/content/sample_data/Fraud.csv path to your file
df = pd.read_csv(r'')

# 3. Convert categorical features into numerical (One-Hot Encoding for 'type')
# IMPORTANT: Replace 'type' with the correct column name from your file
if 'type' in df.columns:
    df = pd.get_dummies(df, columns=['type'], drop_first=True)
else:
    print("⚠️ Column 'type' not found. Skipping One-Hot Encoding.")

# 4. Drop rows with missing values in the target column
# IMPORTANT: Replace 'isFraud' with the correct column name from your file
target_column_name = 'isFraud'  # <--- Replace this with the exact name
if target_column_name in df.columns:
    df.dropna(subset=[target_column_name], inplace=True)
else:
    raise ValueError(f"Column '{target_column_name}' not found in dataset!")

# 5. Select features (X) and target (y)
drop_cols = [target_column_name, 'nameOrig', 'nameDest', 'isFlaggedFraud']
X = df.drop([col for col in drop_cols if col in df.columns], axis=1)
y = df[target_column_name]

# 6. Split into training & testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 7. Create and train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 8. Evaluate the model
accuracy = model.score(X_test, y_test)
print(f"✅ Model Accuracy: {accuracy:.2f}")

# Convert categorical features into numerical ones (e.g., one-hot encoding for 'type')
df = pd.get_dummies(df, columns=['type'], drop_first=True)

# Select features (variables) and the target variable
# This is a critical step that you must justify in your notebook
X = df.drop(['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)
y = df['isFraud']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Choose and train a model
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 1. Import required libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# 2. Load the dataset (assuming you have already uploaded 'Fraud.csv')
df = pd.read_csv('/content/sample_data/Fraud.csv')

# --- Data Cleaning and Preprocessing ---

# 3. Convert categorical features into numerical (One-Hot Encoding for 'type')
df = pd.get_dummies(df, columns=['type'], drop_first=True)

# 4. Handle any potential missing values in the target variable 'isFraud'
# This must be done BEFORE the train-test split.
df.dropna(subset=['isFraud'], inplace=True)

# 5. Select features (X) and target (y)
drop_cols = ['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud']
X = df.drop([col for col in drop_cols if col in df.columns], axis=1)
y = df['isFraud']

# --- Model Development ---

# 6. Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 7. Create and train the Random Forest Classifier model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# --- Model Evaluation and Insights ---

# 8. Predict on the test data
y_pred = model.predict(X_test)

# 9. Evaluate the model's performance
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 10. Get feature importance to identify key factors and visualize it
feature_importance = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 8))
sns.barplot(x=feature_importance, y=feature_importance.index)
plt.title('Feature Importance')
plt.show()
